\section{(Machine) Learning-based ABR algorithms}
We discussed various algorithms that use instantaneous parameters like buffer occupancy and throughput to decide the segment's bitrate. However, researchers have also developed a learning-based algorithm to solve the problem. In this section, we discuss some of those algorithms.

\subsection{Multi-agent Q-Learning}
Petrangeli \etal\ proposes Multi-agent Q-Learning\cite{6838245}, a reinforcement-learning based ABR algorithm. The algorithm does not change the client-side architecture. However, add a co-ordinating proxy server between the server and client. The proxy server collects and aggregates the reward from all the players. Then it computes the global signal from the reward and broadcasts the video to the players. The global signal is used to compute the Homo Egualis\cite{10.5555/1402298.1402344} reward, which is used to provide fairness among the players. With this Homo Egualis reward, players can compute the local reward, which in turn leads to the selection of video quality with the help of throughput, segment lengths, and quality levels.

\subsection{Markov Decision Process optimization}
Gadaleta \etal(\cite{8048013}) and Chiariotti \etal(\cite{10.1145/2910017.2910603}) formulate the Adaptation problem as Markov Decision Process (MDP)\cite{P-1066} optimization problem. Authors have found out that the future state of the process solely depends on the present state. Here, the system's state is defined as quality level, available throughput, size of the segment, and the playback buffer. The state transition happened based on the action taken in the current state based on the action, which is essentially the bitrate. The reward was calculated based on the previous state, action taken in the previous state, and the current state. Chiariotti \etal\ uses online reinforcement learning to solve the problem to make the best possible decision, while Gadaleta \etal\ uses Q-Learning-based deep-neural network to solve the same problem.

\subsection{Pensieve}
Mao \etal\ designed Pensieve\cite{mao2017neural} to solve the bitrate selection problem using Recurrent Neural Network. The Pensieve treats multiple parameters, including buffer occupancy, download history, playback time as the current state, and bitrate selection as the action to move to the next state. Pensieve uses the QoE as the reward for state transition. The aim here is to increase the reward, i.e., QoE, by taking accurate action. Pensieve selection action based on the policy, which a probability distribution of state and action pair. As there are intractably many such pair exists, Pensieve uses a neural network to represent the policy with manageable parameters. It uses the actor-critic neural-network to train the policy with the policy gradient method\cite{sutton1999policy}. It also uses the A3C\cite{10.5555/3045390.3045594} algorithm, an actor-critic method involving two networks, to train its model.\\
Pensieve proposes a simulation-based method to train the network fast, and then the trained model is then used in the real playback system. To make the training process even faster, they propose asynchronous parallel training involving primary and secondary networks.

\subsection{Auto-tuning Video ABR Algorithm (Oboe)}
Oboe\cite{Akhtar2018} is another learning-based ABR technology devised by Akhtar \etal. While Oboe does not directly provide any algorithm to select bitrate for each segment, it tunes the configuration of other ABR algorithms including MPC\cite{yin2015control,10.1145/2670518.2673877}, BOLA\cite{Spiteri2016} and Penseive\cite{mao2017neural} based on the network state. Oboe precomputes the appropriate configuration settings for any algorithm based on the different network states using reinforcement learning. It applies the learned model online to tune the configuration to provide the best possible outcome by an ABR algorithm.


\subsection{Periodical Experience Replay for Multi-path (PERM)}
PERM\cite{9155492} is an actor-critic network-based neural adaptive video streaming system that can exploit the multipath capability of multi-homing devices developed by Guan \etal. PERM uses one actor-network to decide the quality for a segment and another for deciding the path to fetch that segment and uses only one critic-network to evaluate the decision.

\subsection{Super-Resolution based video streaming}
Zhang \etal\ presented a novel approach to increase video streaming quality in their paper \cite{9155384} by improving video quality by the technique call Super-Resolution. Video super-resolution is a technique to reconstruct high-resolution frames from consecutive low-resolution frames. They use an RL-based decision engine to decide the download resolution and whether reconstruction is required or not. The RL-based decision engine is designed based on the state of the actor-critic network.

\subsection{LiveClip}
Most of the ABR algorithms improve video quality for a long video, and they start with the lowest possible video quality. Although it is okay for long videos as the initial part usually not that important, it is a problem for short videos. There are services to stream very short videos. LiveClip\cite{10.1145/3386290.3396937} solves the problem by designing a deep reinforcement learning-based ABR algorithm for short videos. He \etal\ consider a special scenario where the user can scroll through the page to change the video, and the video needs to play whenever it in the visual section. They use adaptive streaming for the entire session instead of per video and adaptation based on the application session (i.e., multiple videos in a session).

\subsection{Overhead-aware Adaptive Video Streaming (Grad)}
Grad\cite{10.1145/3394171.3413512} is overhead-award ABR for SVC based video streaming. By default, DASH is not designed for SVC based video streaming, so most of the ABR algorithm does not support DASH. Also, SVC involves much extra overhead in decoding. Liu \etal\ design grad to mitigate those problems. To reduce decoding overhead, they propose {\tt jump enabled hybrid coding} where only one enhancement layer can be used to jump multiple SVC layers. On top of this optimization, they use actor-critic network-based reinforce-learning to adapt the quality.

\subsection{Summary}
Machine learning and deep learning can perform better than classical systems if trained correctly. The ABR algorithms, like Pensieve, Oboe, show the potential of the machine learning algorithm. The idea of super-resolution and liveClip give another perspective to the video streaming paradigm. While all these ABR algorithms work well in the evaluation, there are not easy to deploy widely. Most of the ABR algorithm needs to train for a long time before they can be used. The training time is enormous and computationally heavy. Training needs to be done frequently and sometimes, per video. Another big hurdle in the deployment of these algorithms is library dependency. To infer the correct decision from an pretrained model, an ML algorithm needs to run in the player. Library supports are important to run ML algorithms in players, and not all the different platforms support all the libraries. Inference can be made by offloading computation to a server. However, it is extremely inefficient as it involves the network. We feel there is much work needed before any of the algorithms can be deployed widely and efficiently.

\begin{table}[h!]
	\resizebox{\textwidth}{!}{
		\begin{tabular}{|l|cccc|c|c|l|l}
			\hline
			\multicolumn{1}{|c|}{\multirow{3}{*}{Algorithm}} & \multicolumn{4}{c|}{Goal involved} & \multirow{3}{*}{\begin{turn}{90}No Extra Module\end{turn}} & \multirow{3}{*}{\begin{turn}{90}Continuous Traffic\end{turn}} & \multicolumn{1}{c|}{\multirow{3}{*}{Remark}} \\
			\cline{2-5}
			& \begin{turn}{90}Rebuffering\end{turn} & \begin{turn}{90}Quality\end{turn} & \begin{turn}{90}Smoothness\end{turn} & \begin{turn}{90}Fairness\end{turn} & & & \\
			& & & & & & & \\
			\hline
			Multi-agent Q-Learning\cite{6838245} & \green{\cmark} & \green{\cmark} & \green{\cmark} & \green{\cmark} & \red{\xmark} & \red{\xmark} & \begin{tabular}[c]{@{}l@{}}Reinforcement Learning\\ Coordination proxy required\end{tabular} \\
			RL-based Online Learning\cite{10.1145/2910017.2910603} & \green{\cmark} & \green{\cmark} & \green{\cmark} & \red{\xmark} & \green{\cmark} & \red{\xmark} & \begin{tabular}[c]{@{}l@{}}Markov decision process\\ Reinforcement learning to solve MDP\end{tabular} \\
			D-DASH \cite{8048013}& \green{\cmark} & \green{\cmark} & \green{\cmark} & \red{\xmark} & \green{\cmark} & \red{\xmark} & \begin{tabular}[c]{@{}l@{}}Markov decision process\\ Deep neural network solve MDP\end{tabular} \\
			Pensieve\cite{mao2017neural} & \green{\cmark} & \green{\cmark} & \green{\cmark} & \red{\xmark} & \green{\cmark} & \red{\xmark} & \begin{tabular}[c]{@{}l@{}}Policy gradient training\\ A3C based actor-critic network\\ Simulation based training\end{tabular} \\
			Oboe\cite{Akhtar2018} & \green{\cmark} & \green{\cmark} & \green{\cmark} & \red{\xmark} & \green{\cmark} & \red{\xmark} & Configuration tuning \\
			PERM\cite{9155492} & \green{\cmark} & \green{\cmark} & \green{\cmark} & \red{\xmark} & \green{\cmark} & \red{\xmark} & \begin{tabular}[c]{@{}l@{}}Path selection problem\\ Actor-critic network with two actor network\end{tabular} \\
			Super-resolution\cite{9155384} & \green{\cmark} & \green{\cmark} & \green{\cmark} & \red{\xmark} & \red{\xmark} & \red{\xmark} & \begin{tabular}[c]{@{}l@{}}Post download quality enhancement\\ Neural network based solution\end{tabular} \\
			\hline
		\end{tabular}
	}
	\caption{\label{chapter02:table:comparison_ml}Comparisons of machine-learning based ABR algorithms}
\end{table}
