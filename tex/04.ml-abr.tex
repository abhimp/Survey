\section{(Machine) Learning-based ABR algorithms}
We discussed various algorithms that use instantaneous parameters like buffer occupancy and throughput to decide the segment's bitrate. However, researchers have also developed a learning-based algorithm to solve the problem. In this section, we discuss some of those algorithms.

\subsection{Multi-agent Q-Learning}
Petrangeli \etal\ proposes Multi-agent Q-Learning\cite{6838245}, a reinforcement-learning based ABR algorithm. The algorithm does not change the client-side architecture. However, add a co-ordinating proxy server between the server and client. The proxy server collects and aggregates the reward from all the players. Then it computes the global signal from the reward and broadcasts to the players. The global signal is used to compute the Homo Egualis\cite{10.5555/1402298.1402344} reward, which is used to provide fairness among the players. With this Homo Egualis reward, players can compute the local reward, which in turn leads to the selection of video quality with the help of throughput, segment lengths, and quality levels.

\subsection{RL based Online Learning Adaptation}
Chiariotti \etal\ formulate the Adaptation problem as Markov Decision Process (MDP)\cite{P-1066} optimization problem\cite{10.1145/2910017.2910603}. Authors have found out that the future state of the process solely depends on the present state. Here, the system's state is defined as quality level, available throughput, size of the segment, and the playback buffer. The state transition happened based on the action taken in the current state based on the action, which is essentially the bitrate. The reward was calculated based on the previous state, action taken in the previous state, and current state. Then they use online reinforcement learning to solving the problem to make the best possible decision.

\subsection{D-DASH \cite{8048013}}
D-DASH\cite{8048013} is a deep-neural network-based ABR solution. Like \cite{10.1145/2910017.2910603}, Gadaleta \etal\ formulate the problem as MDP optimization. However, the authors suggested a Q-Learning-based deep-neural network solution instead of an RL-based solution as it is compact and fast.

\subsection{Pensieve}
Mao \etal\ designed Pensieve\cite{10.1145/3098822.3098843} to solve the bitrate selection problem using Recurrent Neural Network. The Pensieve treats multiple parameters, including buffer occupancy, download history, playback time as the current state, and bitrate selection as the action to move to the next state. Pensieve uses the QoE as the reward for state transition. The aim here is to increase the reward, i.e., QoE, by taking accurate action. Pensieve selection action based on the policy, which a probability distribution of state and action pair. As there are intractably many such pair exists, Pensieve uses a neural network to represent the policy with manageable parameters. It uses the actor-critic neural-network to train the policy with the policy gradient method\cite{sutton1999policy}. It also uses the A3C\cite{10.5555/3045390.3045594} algorithm, an actor-critic method involving two networks, to train its model.\\
Pensieve proposes a simulation-based method to train the network fast, and then the trained model is then used in the real playback system. To make the training process even faster, they propose asynchronous parallel training involving primary and secondary networks.

\subsection{Oboe}
Oboe\cite{10.1145/3230543.3230558} is another learning-based ABR technology devised by Akhtar \etal. While Oboe does not directly provide any algorithm to select bitrate for each segment, it tunes the configuration of other ABR algorithms including MPC\cite{10.1145/2785956.2787486,10.1145/2670518.2673877}, BOLA\cite{7524428} and Penseive\cite{10.1145/3098822.3098843} based on the network state. Oboe precomputes the appropriate configuration settings for any algorithm based on the different network states using reinforcement learning. It applies the learned model online to tune the configuration to provide the best possible outcome by an ABR algorithm.


\subsection{Periodical Experience Replay for Multi-path (PERM)}
PERM\cite{9155492} is an actor-critic network-based neural adaptive video streaming system that can exploit the multipath capability of multi-homing devices developed by Guan \etal. PERM uses one actor-network to decide the quality for a segment and another for deciding the path to fetch that segment and uses only one critic-network to evaluate the decision.

\subsection{Super-Resolution based video streaming}
Zhang \etal\ presented a novel approach to increase video streaming quality in their paper \cite{9155384} by improving video quality by the technique call Super-Resolution. Super-Resolution is a technique where clients can reconstruct a high-quality video from low-quality videos using reinforcement learning.

\subsection{LiveClip}
When we talk about DASH based videos, we generally think of long videos. So, most of the ABR algorithm might not work for short videos. However, some services use short videos. LiveClip\cite{10.1145/3386290.3396937} solves the problem by designing a deep reinforcement learning-based ABR algorithm for short videos.

\subsection{Grad}
Grad\cite{10.1145/3394171.3413512} is overhead-award ABR for SVC based video streaming. By default, DASH is not designed for SVC based video streaming, so most of the ABR algorithm does not support DASH. Also, SVC involves a lot of extra overhead in decoding. Liu \etal\ design grad to mitigate those problems. To reduce decoding overhead, they propose {\tt jump enabled hybrid coding} where only one enhancement layer can be used to jump multiple SVC layers. On top of this optimization, they use actor-critic network-based reinforce-learning to adapt the quality.